{"ast":null,"code":"'use strict';\n\nconst BB = require('bluebird');\nconst contentPath = require('./content/path');\nconst crypto = require('crypto');\nconst figgyPudding = require('figgy-pudding');\nconst fixOwner = require('./util/fix-owner');\nconst fs = require('graceful-fs');\nconst hashToSegments = require('./util/hash-to-segments');\nconst ms = require('mississippi');\nconst path = require('path');\nconst ssri = require('ssri');\nconst Y = require('./util/y.js');\nconst indexV = require('../package.json')['cache-version'].index;\nconst appendFileAsync = BB.promisify(fs.appendFile);\nconst readFileAsync = BB.promisify(fs.readFile);\nconst readdirAsync = BB.promisify(fs.readdir);\nconst concat = ms.concat;\nconst from = ms.from;\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor(cache, key) {\n    super(Y`No cache entry for \\`${key}\\` found in \\`${cache}\\``);\n    this.code = 'ENOENT';\n    this.cache = cache;\n    this.key = key;\n  }\n};\nconst IndexOpts = figgyPudding({\n  metadata: {},\n  size: {}\n});\nmodule.exports.insert = insert;\nfunction insert(cache, key, integrity, opts) {\n  opts = IndexOpts(opts);\n  const bucket = bucketPath(cache, key);\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size: opts.size,\n    metadata: opts.metadata\n  };\n  return fixOwner.mkdirfix(cache, path.dirname(bucket)).then(() => {\n    const stringified = JSON.stringify(entry);\n    // NOTE - Cleverness ahoy!\n    //\n    // This works because it's tremendously unlikely for an entry to corrupt\n    // another while still preserving the string length of the JSON in\n    // question. So, we just slap the length in there and verify it on read.\n    //\n    // Thanks to @isaacs for the whiteboarding session that ended up with this.\n    return appendFileAsync(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n  }).then(() => fixOwner.chownr(cache, bucket)).catch({\n    code: 'ENOENT'\n  }, () => {\n    // There's a class of race conditions that happen when things get deleted\n    // during fixOwner, or between the two mkdirfix/chownr calls.\n    //\n    // It's perfectly fine to just not bother in those cases and lie\n    // that the index entry was written. Because it's a cache.\n  }).then(() => {\n    return formatEntry(cache, entry);\n  });\n}\nmodule.exports.insert.sync = insertSync;\nfunction insertSync(cache, key, integrity, opts) {\n  opts = IndexOpts(opts);\n  const bucket = bucketPath(cache, key);\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size: opts.size,\n    metadata: opts.metadata\n  };\n  fixOwner.mkdirfix.sync(cache, path.dirname(bucket));\n  const stringified = JSON.stringify(entry);\n  fs.appendFileSync(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n  try {\n    fixOwner.chownr.sync(cache, bucket);\n  } catch (err) {\n    if (err.code !== 'ENOENT') {\n      throw err;\n    }\n  }\n  return formatEntry(cache, entry);\n}\nmodule.exports.find = find;\nfunction find(cache, key) {\n  const bucket = bucketPath(cache, key);\n  return bucketEntries(bucket).then(entries => {\n    return entries.reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next);\n      } else {\n        return latest;\n      }\n    }, null);\n  }).catch(err => {\n    if (err.code === 'ENOENT') {\n      return null;\n    } else {\n      throw err;\n    }\n  });\n}\nmodule.exports.find.sync = findSync;\nfunction findSync(cache, key) {\n  const bucket = bucketPath(cache, key);\n  try {\n    return bucketEntriesSync(bucket).reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next);\n      } else {\n        return latest;\n      }\n    }, null);\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null;\n    } else {\n      throw err;\n    }\n  }\n}\nmodule.exports.delete = del;\nfunction del(cache, key, opts) {\n  return insert(cache, key, null, opts);\n}\nmodule.exports.delete.sync = delSync;\nfunction delSync(cache, key, opts) {\n  return insertSync(cache, key, null, opts);\n}\nmodule.exports.lsStream = lsStream;\nfunction lsStream(cache) {\n  const indexDir = bucketDir(cache);\n  const stream = from.obj();\n\n  // \"/cachename/*\"\n  readdirOrEmpty(indexDir).map(bucket => {\n    const bucketPath = path.join(indexDir, bucket);\n\n    // \"/cachename/<bucket 0xFF>/*\"\n    return readdirOrEmpty(bucketPath).map(subbucket => {\n      const subbucketPath = path.join(bucketPath, subbucket);\n\n      // \"/cachename/<bucket 0xFF>/<bucket 0xFF>/*\"\n      return readdirOrEmpty(subbucketPath).map(entry => {\n        const getKeyToEntry = bucketEntries(path.join(subbucketPath, entry)).reduce((acc, entry) => {\n          acc.set(entry.key, entry);\n          return acc;\n        }, new Map());\n        return getKeyToEntry.then(reduced => {\n          for (let entry of reduced.values()) {\n            const formatted = formatEntry(cache, entry);\n            formatted && stream.push(formatted);\n          }\n        }).catch({\n          code: 'ENOENT'\n        }, nop);\n      });\n    });\n  }).then(() => {\n    stream.push(null);\n  }, err => {\n    stream.emit('error', err);\n  });\n  return stream;\n}\nmodule.exports.ls = ls;\nfunction ls(cache) {\n  return BB.fromNode(cb => {\n    lsStream(cache).on('error', cb).pipe(concat(entries => {\n      cb(null, entries.reduce((acc, xs) => {\n        acc[xs.key] = xs;\n        return acc;\n      }, {}));\n    }));\n  });\n}\nfunction bucketEntries(bucket, filter) {\n  return readFileAsync(bucket, 'utf8').then(data => _bucketEntries(data, filter));\n}\nfunction bucketEntriesSync(bucket, filter) {\n  const data = fs.readFileSync(bucket, 'utf8');\n  return _bucketEntries(data, filter);\n}\nfunction _bucketEntries(data, filter) {\n  let entries = [];\n  data.split('\\n').forEach(entry => {\n    if (!entry) {\n      return;\n    }\n    const pieces = entry.split('\\t');\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return;\n    }\n    let obj;\n    try {\n      obj = JSON.parse(pieces[1]);\n    } catch (e) {\n      // Entry is corrupted!\n      return;\n    }\n    if (obj) {\n      entries.push(obj);\n    }\n  });\n  return entries;\n}\nmodule.exports._bucketDir = bucketDir;\nfunction bucketDir(cache) {\n  return path.join(cache, `index-v${indexV}`);\n}\nmodule.exports._bucketPath = bucketPath;\nfunction bucketPath(cache, key) {\n  const hashed = hashKey(key);\n  return path.join.apply(path, [bucketDir(cache)].concat(hashToSegments(hashed)));\n}\nmodule.exports._hashKey = hashKey;\nfunction hashKey(key) {\n  return hash(key, 'sha256');\n}\nmodule.exports._hashEntry = hashEntry;\nfunction hashEntry(str) {\n  return hash(str, 'sha1');\n}\nfunction hash(str, digest) {\n  return crypto.createHash(digest).update(str).digest('hex');\n}\nfunction formatEntry(cache, entry) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity) {\n    return null;\n  }\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: contentPath(cache, entry.integrity),\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata\n  };\n}\nfunction readdirOrEmpty(dir) {\n  return readdirAsync(dir).catch({\n    code: 'ENOENT'\n  }, () => []).catch({\n    code: 'ENOTDIR'\n  }, () => []);\n}\nfunction nop() {}","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}