{"ast":null,"code":"/*\n\tMIT License http://www.opensource.org/licenses/mit-license.php\n\tAuthor Tobias Koppers @sokra\n*/\n\"use strict\";\n\nfunction _slicedToArray(arr, i) { return _arrayWithHoles(arr) || _iterableToArrayLimit(arr, i) || _unsupportedIterableToArray(arr, i) || _nonIterableRest(); }\nfunction _nonIterableRest() { throw new TypeError(\"Invalid attempt to destructure non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); }\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) arr2[i] = arr[i]; return arr2; }\nfunction _iterableToArrayLimit(arr, i) { var _i = null == arr ? null : \"undefined\" != typeof Symbol && arr[Symbol.iterator] || arr[\"@@iterator\"]; if (null != _i) { var _s, _e, _x, _r, _arr = [], _n = !0, _d = !1; try { if (_x = (_i = _i.call(arr)).next, 0 === i) { if (Object(_i) !== _i) return; _n = !1; } else for (; !(_n = (_s = _x.call(_i)).done) && (_arr.push(_s.value), _arr.length !== i); _n = !0); } catch (err) { _d = !0, _e = err; } finally { try { if (!_n && null != _i.return && (_r = _i.return(), Object(_r) !== _r)) return; } finally { if (_d) throw _e; } } return _arr; } }\nfunction _arrayWithHoles(arr) { if (Array.isArray(arr)) return arr; }\nconst crypto = require(\"crypto\");\nconst SortableSet = require(\"../util/SortableSet\");\nconst GraphHelpers = require(\"../GraphHelpers\");\nconst _require = require(\"../util/SetHelpers\"),\n  isSubset = _require.isSubset;\nconst deterministicGrouping = require(\"../util/deterministicGrouping\");\nconst MinMaxSizeWarning = require(\"./MinMaxSizeWarning\");\nconst contextify = require(\"../util/identifier\").contextify;\n\n/** @typedef {import(\"../Compiler\")} Compiler */\n/** @typedef {import(\"../Chunk\")} Chunk */\n/** @typedef {import(\"../Module\")} Module */\n/** @typedef {import(\"../util/deterministicGrouping\").Options<Module>} DeterministicGroupingOptionsForModule */\n/** @typedef {import(\"../util/deterministicGrouping\").GroupedItems<Module>} DeterministicGroupingGroupedItemsForModule */\n\nconst deterministicGroupingForModules = /** @type {function(DeterministicGroupingOptionsForModule): DeterministicGroupingGroupedItemsForModule[]} */deterministicGrouping;\nconst hashFilename = name => {\n  return crypto.createHash(\"md4\").update(name).digest(\"hex\").slice(0, 8);\n};\nconst sortByIdentifier = (a, b) => {\n  if (a.identifier() > b.identifier()) return 1;\n  if (a.identifier() < b.identifier()) return -1;\n  return 0;\n};\nconst getRequests = chunk => {\n  let requests = 0;\n  for (const chunkGroup of chunk.groupsIterable) {\n    requests = Math.max(requests, chunkGroup.chunks.length);\n  }\n  return requests;\n};\nconst getModulesSize = modules => {\n  let sum = 0;\n  for (const m of modules) {\n    sum += m.size();\n  }\n  return sum;\n};\n\n/**\n * @template T\n * @param {Set<T>} a set\n * @param {Set<T>} b other set\n * @returns {boolean} true if at least one item of a is in b\n */\nconst isOverlap = (a, b) => {\n  for (const item of a) {\n    if (b.has(item)) return true;\n  }\n  return false;\n};\nconst compareEntries = (a, b) => {\n  // 1. by priority\n  const diffPriority = a.cacheGroup.priority - b.cacheGroup.priority;\n  if (diffPriority) return diffPriority;\n  // 2. by number of chunks\n  const diffCount = a.chunks.size - b.chunks.size;\n  if (diffCount) return diffCount;\n  // 3. by size reduction\n  const aSizeReduce = a.size * (a.chunks.size - 1);\n  const bSizeReduce = b.size * (b.chunks.size - 1);\n  const diffSizeReduce = aSizeReduce - bSizeReduce;\n  if (diffSizeReduce) return diffSizeReduce;\n  // 4. by cache group index\n  const indexDiff = b.cacheGroupIndex - a.cacheGroupIndex;\n  if (indexDiff) return indexDiff;\n  // 5. by number of modules (to be able to compare by identifier)\n  const modulesA = a.modules;\n  const modulesB = b.modules;\n  const diff = modulesA.size - modulesB.size;\n  if (diff) return diff;\n  // 6. by module identifiers\n  modulesA.sort();\n  modulesB.sort();\n  const aI = modulesA[Symbol.iterator]();\n  const bI = modulesB[Symbol.iterator]();\n  // eslint-disable-next-line no-constant-condition\n  while (true) {\n    const aItem = aI.next();\n    const bItem = bI.next();\n    if (aItem.done) return 0;\n    const aModuleIdentifier = aItem.value.identifier();\n    const bModuleIdentifier = bItem.value.identifier();\n    if (aModuleIdentifier > bModuleIdentifier) return -1;\n    if (aModuleIdentifier < bModuleIdentifier) return 1;\n  }\n};\nconst compareNumbers = (a, b) => a - b;\nconst INITIAL_CHUNK_FILTER = chunk => chunk.canBeInitial();\nconst ASYNC_CHUNK_FILTER = chunk => !chunk.canBeInitial();\nconst ALL_CHUNK_FILTER = chunk => true;\nmodule.exports = class SplitChunksPlugin {\n  constructor(options) {\n    this.options = SplitChunksPlugin.normalizeOptions(options);\n  }\n  static normalizeOptions() {\n    let options = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    return {\n      chunksFilter: SplitChunksPlugin.normalizeChunksFilter(options.chunks || \"all\"),\n      minSize: options.minSize || 0,\n      enforceSizeThreshold: options.enforceSizeThreshold || 0,\n      maxSize: options.maxSize || 0,\n      minChunks: options.minChunks || 1,\n      maxAsyncRequests: options.maxAsyncRequests || 1,\n      maxInitialRequests: options.maxInitialRequests || 1,\n      hidePathInfo: options.hidePathInfo || false,\n      filename: options.filename || undefined,\n      getCacheGroups: SplitChunksPlugin.normalizeCacheGroups({\n        cacheGroups: options.cacheGroups,\n        name: options.name,\n        automaticNameDelimiter: options.automaticNameDelimiter,\n        automaticNameMaxLength: options.automaticNameMaxLength\n      }),\n      automaticNameDelimiter: options.automaticNameDelimiter,\n      automaticNameMaxLength: options.automaticNameMaxLength || 109,\n      fallbackCacheGroup: SplitChunksPlugin.normalizeFallbackCacheGroup(options.fallbackCacheGroup || {}, options)\n    };\n  }\n  static normalizeName(_ref) {\n    let name = _ref.name,\n      automaticNameDelimiter = _ref.automaticNameDelimiter,\n      automaticNamePrefix = _ref.automaticNamePrefix,\n      automaticNameMaxLength = _ref.automaticNameMaxLength;\n    if (name === true) {\n      /** @type {WeakMap<Chunk[], Record<string, string>>} */\n      const cache = new WeakMap();\n      const fn = (module, chunks, cacheGroup) => {\n        let cacheEntry = cache.get(chunks);\n        if (cacheEntry === undefined) {\n          cacheEntry = {};\n          cache.set(chunks, cacheEntry);\n        } else if (cacheGroup in cacheEntry) {\n          return cacheEntry[cacheGroup];\n        }\n        const names = chunks.map(c => c.name);\n        if (!names.every(Boolean)) {\n          cacheEntry[cacheGroup] = undefined;\n          return;\n        }\n        names.sort();\n        const prefix = typeof automaticNamePrefix === \"string\" ? automaticNamePrefix : cacheGroup;\n        const namePrefix = prefix ? prefix + automaticNameDelimiter : \"\";\n        let name = namePrefix + names.join(automaticNameDelimiter);\n        // Filenames and paths can't be too long otherwise an\n        // ENAMETOOLONG error is raised. If the generated name if too\n        // long, it is truncated and a hash is appended. The limit has\n        // been set to 109 to prevent `[name].[chunkhash].[ext]` from\n        // generating a 256+ character string.\n        if (name.length > automaticNameMaxLength) {\n          const hashedFilename = hashFilename(name);\n          const sliceLength = automaticNameMaxLength - (automaticNameDelimiter.length + hashedFilename.length);\n          name = name.slice(0, sliceLength) + automaticNameDelimiter + hashedFilename;\n        }\n        cacheEntry[cacheGroup] = name;\n        return name;\n      };\n      return fn;\n    }\n    if (typeof name === \"string\") {\n      const fn = () => {\n        return name;\n      };\n      return fn;\n    }\n    if (typeof name === \"function\") return name;\n  }\n  static normalizeChunksFilter(chunks) {\n    if (chunks === \"initial\") {\n      return INITIAL_CHUNK_FILTER;\n    }\n    if (chunks === \"async\") {\n      return ASYNC_CHUNK_FILTER;\n    }\n    if (chunks === \"all\") {\n      return ALL_CHUNK_FILTER;\n    }\n    if (typeof chunks === \"function\") return chunks;\n  }\n  static normalizeFallbackCacheGroup(_ref2, _ref3) {\n    let _ref2$minSize = _ref2.minSize,\n      minSize = _ref2$minSize === void 0 ? undefined : _ref2$minSize,\n      _ref2$maxSize = _ref2.maxSize,\n      maxSize = _ref2$maxSize === void 0 ? undefined : _ref2$maxSize,\n      _ref2$automaticNameDe = _ref2.automaticNameDelimiter,\n      automaticNameDelimiter = _ref2$automaticNameDe === void 0 ? undefined : _ref2$automaticNameDe;\n    let _ref3$minSize = _ref3.minSize,\n      defaultMinSize = _ref3$minSize === void 0 ? undefined : _ref3$minSize,\n      _ref3$maxSize = _ref3.maxSize,\n      defaultMaxSize = _ref3$maxSize === void 0 ? undefined : _ref3$maxSize,\n      _ref3$automaticNameDe = _ref3.automaticNameDelimiter,\n      defaultAutomaticNameDelimiter = _ref3$automaticNameDe === void 0 ? undefined : _ref3$automaticNameDe;\n    return {\n      minSize: typeof minSize === \"number\" ? minSize : defaultMinSize || 0,\n      maxSize: typeof maxSize === \"number\" ? maxSize : defaultMaxSize || 0,\n      automaticNameDelimiter: automaticNameDelimiter || defaultAutomaticNameDelimiter || \"~\"\n    };\n  }\n  static normalizeCacheGroups(_ref4) {\n    let cacheGroups = _ref4.cacheGroups,\n      name = _ref4.name,\n      automaticNameDelimiter = _ref4.automaticNameDelimiter,\n      automaticNameMaxLength = _ref4.automaticNameMaxLength;\n    if (typeof cacheGroups === \"function\") {\n      // TODO webpack 5 remove this\n      if (cacheGroups.length !== 1) {\n        return module => cacheGroups(module, module.getChunks());\n      }\n      return cacheGroups;\n    }\n    if (cacheGroups && typeof cacheGroups === \"object\") {\n      const fn = module => {\n        let results;\n        for (const key of Object.keys(cacheGroups)) {\n          let option = cacheGroups[key];\n          if (option === false) continue;\n          if (option instanceof RegExp || typeof option === \"string\") {\n            option = {\n              test: option\n            };\n          }\n          if (typeof option === \"function\") {\n            let result = option(module);\n            if (result) {\n              if (results === undefined) results = [];\n              for (const r of Array.isArray(result) ? result : [result]) {\n                const result = Object.assign({\n                  key\n                }, r);\n                if (result.name) result.getName = () => result.name;\n                if (result.chunks) {\n                  result.chunksFilter = SplitChunksPlugin.normalizeChunksFilter(result.chunks);\n                }\n                results.push(result);\n              }\n            }\n          } else if (SplitChunksPlugin.checkTest(option.test, module)) {\n            if (results === undefined) results = [];\n            results.push({\n              key: key,\n              priority: option.priority,\n              getName: SplitChunksPlugin.normalizeName({\n                name: option.name || name,\n                automaticNameDelimiter: typeof option.automaticNameDelimiter === \"string\" ? option.automaticNameDelimiter : automaticNameDelimiter,\n                automaticNamePrefix: option.automaticNamePrefix,\n                automaticNameMaxLength: option.automaticNameMaxLength || automaticNameMaxLength\n              }) || (() => {}),\n              chunksFilter: SplitChunksPlugin.normalizeChunksFilter(option.chunks),\n              enforce: option.enforce,\n              minSize: option.minSize,\n              enforceSizeThreshold: option.enforceSizeThreshold,\n              maxSize: option.maxSize,\n              minChunks: option.minChunks,\n              maxAsyncRequests: option.maxAsyncRequests,\n              maxInitialRequests: option.maxInitialRequests,\n              filename: option.filename,\n              reuseExistingChunk: option.reuseExistingChunk\n            });\n          }\n        }\n        return results;\n      };\n      return fn;\n    }\n    const fn = () => {};\n    return fn;\n  }\n  static checkTest(test, module) {\n    if (test === undefined) return true;\n    if (typeof test === \"function\") {\n      if (test.length !== 1) {\n        return test(module, module.getChunks());\n      }\n      return test(module);\n    }\n    if (typeof test === \"boolean\") return test;\n    if (typeof test === \"string\") {\n      if (module.nameForCondition && module.nameForCondition().startsWith(test)) {\n        return true;\n      }\n      for (const chunk of module.chunksIterable) {\n        if (chunk.name && chunk.name.startsWith(test)) {\n          return true;\n        }\n      }\n      return false;\n    }\n    if (test instanceof RegExp) {\n      if (module.nameForCondition && test.test(module.nameForCondition())) {\n        return true;\n      }\n      for (const chunk of module.chunksIterable) {\n        if (chunk.name && test.test(chunk.name)) {\n          return true;\n        }\n      }\n      return false;\n    }\n    return false;\n  }\n\n  /**\n   * @param {Compiler} compiler webpack compiler\n   * @returns {void}\n   */\n  apply(compiler) {\n    compiler.hooks.thisCompilation.tap(\"SplitChunksPlugin\", compilation => {\n      let alreadyOptimized = false;\n      compilation.hooks.unseal.tap(\"SplitChunksPlugin\", () => {\n        alreadyOptimized = false;\n      });\n      compilation.hooks.optimizeChunksAdvanced.tap(\"SplitChunksPlugin\", chunks => {\n        if (alreadyOptimized) return;\n        alreadyOptimized = true;\n        // Give each selected chunk an index (to create strings from chunks)\n        const indexMap = new Map();\n        let index = 1;\n        for (const chunk of chunks) {\n          indexMap.set(chunk, index++);\n        }\n        const getKey = chunks => {\n          return Array.from(chunks, c => indexMap.get(c)).sort(compareNumbers).join();\n        };\n        /** @type {Map<string, Set<Chunk>>} */\n        const chunkSetsInGraph = new Map();\n        for (const module of compilation.modules) {\n          const chunksKey = getKey(module.chunksIterable);\n          if (!chunkSetsInGraph.has(chunksKey)) {\n            chunkSetsInGraph.set(chunksKey, new Set(module.chunksIterable));\n          }\n        }\n\n        // group these set of chunks by count\n        // to allow to check less sets via isSubset\n        // (only smaller sets can be subset)\n        /** @type {Map<number, Array<Set<Chunk>>>} */\n        const chunkSetsByCount = new Map();\n        for (const chunksSet of chunkSetsInGraph.values()) {\n          const count = chunksSet.size;\n          let array = chunkSetsByCount.get(count);\n          if (array === undefined) {\n            array = [];\n            chunkSetsByCount.set(count, array);\n          }\n          array.push(chunksSet);\n        }\n\n        // Create a list of possible combinations\n        const combinationsCache = new Map(); // Map<string, Set<Chunk>[]>\n\n        const getCombinations = key => {\n          const chunksSet = chunkSetsInGraph.get(key);\n          var array = [chunksSet];\n          if (chunksSet.size > 1) {\n            for (const _ref5 of chunkSetsByCount) {\n              var _ref6 = _slicedToArray(_ref5, 2);\n              const count = _ref6[0];\n              const setArray = _ref6[1];\n              // \"equal\" is not needed because they would have been merge in the first step\n              if (count < chunksSet.size) {\n                for (const set of setArray) {\n                  if (isSubset(chunksSet, set)) {\n                    array.push(set);\n                  }\n                }\n              }\n            }\n          }\n          return array;\n        };\n\n        /**\n         * @typedef {Object} SelectedChunksResult\n         * @property {Chunk[]} chunks the list of chunks\n         * @property {string} key a key of the list\n         */\n\n        /**\n         * @typedef {function(Chunk): boolean} ChunkFilterFunction\n         */\n\n        /** @type {WeakMap<Set<Chunk>, WeakMap<ChunkFilterFunction, SelectedChunksResult>>} */\n        const selectedChunksCacheByChunksSet = new WeakMap();\n\n        /**\n         * get list and key by applying the filter function to the list\n         * It is cached for performance reasons\n         * @param {Set<Chunk>} chunks list of chunks\n         * @param {ChunkFilterFunction} chunkFilter filter function for chunks\n         * @returns {SelectedChunksResult} list and key\n         */\n        const getSelectedChunks = (chunks, chunkFilter) => {\n          let entry = selectedChunksCacheByChunksSet.get(chunks);\n          if (entry === undefined) {\n            entry = new WeakMap();\n            selectedChunksCacheByChunksSet.set(chunks, entry);\n          }\n          /** @type {SelectedChunksResult} */\n          let entry2 = entry.get(chunkFilter);\n          if (entry2 === undefined) {\n            /** @type {Chunk[]} */\n            const selectedChunks = [];\n            for (const chunk of chunks) {\n              if (chunkFilter(chunk)) selectedChunks.push(chunk);\n            }\n            entry2 = {\n              chunks: selectedChunks,\n              key: getKey(selectedChunks)\n            };\n            entry.set(chunkFilter, entry2);\n          }\n          return entry2;\n        };\n\n        /**\n         * @typedef {Object} ChunksInfoItem\n         * @property {SortableSet} modules\n         * @property {TODO} cacheGroup\n         * @property {number} cacheGroupIndex\n         * @property {string} name\n         * @property {number} size\n         * @property {Set<Chunk>} chunks\n         * @property {Set<Chunk>} reuseableChunks\n         * @property {Set<string>} chunksKeys\n         */\n\n        // Map a list of chunks to a list of modules\n        // For the key the chunk \"index\" is used, the value is a SortableSet of modules\n        /** @type {Map<string, ChunksInfoItem>} */\n        const chunksInfoMap = new Map();\n\n        /**\n         * @param {TODO} cacheGroup the current cache group\n         * @param {number} cacheGroupIndex the index of the cache group of ordering\n         * @param {Chunk[]} selectedChunks chunks selected for this module\n         * @param {string} selectedChunksKey a key of selectedChunks\n         * @param {Module} module the current module\n         * @returns {void}\n         */\n        const addModuleToChunksInfoMap = (cacheGroup, cacheGroupIndex, selectedChunks, selectedChunksKey, module) => {\n          // Break if minimum number of chunks is not reached\n          if (selectedChunks.length < cacheGroup.minChunks) return;\n          // Determine name for split chunk\n          const name = cacheGroup.getName(module, selectedChunks, cacheGroup.key);\n          // Create key for maps\n          // When it has a name we use the name as key\n          // Elsewise we create the key from chunks and cache group key\n          // This automatically merges equal names\n          const key = cacheGroup.key + (name ? ` name:${name}` : ` chunks:${selectedChunksKey}`);\n          // Add module to maps\n          let info = chunksInfoMap.get(key);\n          if (info === undefined) {\n            chunksInfoMap.set(key, info = {\n              modules: new SortableSet(undefined, sortByIdentifier),\n              cacheGroup,\n              cacheGroupIndex,\n              name,\n              size: 0,\n              chunks: new Set(),\n              reuseableChunks: new Set(),\n              chunksKeys: new Set()\n            });\n          }\n          const oldSize = info.modules.size;\n          info.modules.add(module);\n          if (info.modules.size !== oldSize) {\n            info.size += module.size();\n          }\n          const oldChunksKeysSize = info.chunksKeys.size;\n          info.chunksKeys.add(selectedChunksKey);\n          if (oldChunksKeysSize !== info.chunksKeys.size) {\n            for (const chunk of selectedChunks) {\n              info.chunks.add(chunk);\n            }\n          }\n        };\n\n        // Walk through all modules\n        for (const module of compilation.modules) {\n          // Get cache group\n          let cacheGroups = this.options.getCacheGroups(module);\n          if (!Array.isArray(cacheGroups) || cacheGroups.length === 0) {\n            continue;\n          }\n\n          // Prepare some values\n          const chunksKey = getKey(module.chunksIterable);\n          let combs = combinationsCache.get(chunksKey);\n          if (combs === undefined) {\n            combs = getCombinations(chunksKey);\n            combinationsCache.set(chunksKey, combs);\n          }\n          let cacheGroupIndex = 0;\n          for (const cacheGroupSource of cacheGroups) {\n            const minSize = cacheGroupSource.minSize !== undefined ? cacheGroupSource.minSize : cacheGroupSource.enforce ? 0 : this.options.minSize;\n            const enforceSizeThreshold = cacheGroupSource.enforceSizeThreshold !== undefined ? cacheGroupSource.enforceSizeThreshold : cacheGroupSource.enforce ? 0 : this.options.enforceSizeThreshold;\n            const cacheGroup = {\n              key: cacheGroupSource.key,\n              priority: cacheGroupSource.priority || 0,\n              chunksFilter: cacheGroupSource.chunksFilter || this.options.chunksFilter,\n              minSize,\n              minSizeForMaxSize: cacheGroupSource.minSize !== undefined ? cacheGroupSource.minSize : this.options.minSize,\n              enforceSizeThreshold,\n              maxSize: cacheGroupSource.maxSize !== undefined ? cacheGroupSource.maxSize : cacheGroupSource.enforce ? 0 : this.options.maxSize,\n              minChunks: cacheGroupSource.minChunks !== undefined ? cacheGroupSource.minChunks : cacheGroupSource.enforce ? 1 : this.options.minChunks,\n              maxAsyncRequests: cacheGroupSource.maxAsyncRequests !== undefined ? cacheGroupSource.maxAsyncRequests : cacheGroupSource.enforce ? Infinity : this.options.maxAsyncRequests,\n              maxInitialRequests: cacheGroupSource.maxInitialRequests !== undefined ? cacheGroupSource.maxInitialRequests : cacheGroupSource.enforce ? Infinity : this.options.maxInitialRequests,\n              getName: cacheGroupSource.getName !== undefined ? cacheGroupSource.getName : this.options.getName,\n              filename: cacheGroupSource.filename !== undefined ? cacheGroupSource.filename : this.options.filename,\n              automaticNameDelimiter: cacheGroupSource.automaticNameDelimiter !== undefined ? cacheGroupSource.automaticNameDelimiter : this.options.automaticNameDelimiter,\n              reuseExistingChunk: cacheGroupSource.reuseExistingChunk,\n              _validateSize: minSize > 0,\n              _conditionalEnforce: enforceSizeThreshold > 0\n            };\n            // For all combination of chunk selection\n            for (const chunkCombination of combs) {\n              // Break if minimum number of chunks is not reached\n              if (chunkCombination.size < cacheGroup.minChunks) continue;\n              // Select chunks by configuration\n              const _getSelectedChunks = getSelectedChunks(chunkCombination, cacheGroup.chunksFilter),\n                selectedChunks = _getSelectedChunks.chunks,\n                selectedChunksKey = _getSelectedChunks.key;\n              addModuleToChunksInfoMap(cacheGroup, cacheGroupIndex, selectedChunks, selectedChunksKey, module);\n            }\n            cacheGroupIndex++;\n          }\n        }\n\n        // Filter items were size < minSize\n        for (const pair of chunksInfoMap) {\n          const info = pair[1];\n          if (info.cacheGroup._validateSize && info.size < info.cacheGroup.minSize) {\n            chunksInfoMap.delete(pair[0]);\n          }\n        }\n\n        /** @type {Map<Chunk, {minSize: number, maxSize: number, automaticNameDelimiter: string, keys: string[]}>} */\n        const maxSizeQueueMap = new Map();\n        while (chunksInfoMap.size > 0) {\n          // Find best matching entry\n          let bestEntryKey;\n          let bestEntry;\n          for (const pair of chunksInfoMap) {\n            const key = pair[0];\n            const info = pair[1];\n            if (bestEntry === undefined) {\n              bestEntry = info;\n              bestEntryKey = key;\n            } else if (compareEntries(bestEntry, info) < 0) {\n              bestEntry = info;\n              bestEntryKey = key;\n            }\n          }\n          const item = bestEntry;\n          chunksInfoMap.delete(bestEntryKey);\n          let chunkName = item.name;\n          // Variable for the new chunk (lazy created)\n          /** @type {Chunk} */\n          let newChunk;\n          // When no chunk name, check if we can reuse a chunk instead of creating a new one\n          let isReused = false;\n          if (item.cacheGroup.reuseExistingChunk) {\n            outer: for (const chunk of item.chunks) {\n              if (chunk.getNumberOfModules() !== item.modules.size) continue;\n              if (chunk.hasEntryModule()) continue;\n              for (const module of item.modules) {\n                if (!chunk.containsModule(module)) continue outer;\n              }\n              if (!newChunk || !newChunk.name) {\n                newChunk = chunk;\n              } else if (chunk.name && chunk.name.length < newChunk.name.length) {\n                newChunk = chunk;\n              } else if (chunk.name && chunk.name.length === newChunk.name.length && chunk.name < newChunk.name) {\n                newChunk = chunk;\n              }\n              chunkName = undefined;\n              isReused = true;\n            }\n          }\n          // Check if maxRequests condition can be fulfilled\n\n          const selectedChunks = Array.from(item.chunks).filter(chunk => {\n            // skip if we address ourself\n            return (!chunkName || chunk.name !== chunkName) && chunk !== newChunk;\n          });\n          const enforced = item.cacheGroup._conditionalEnforce && item.size >= item.cacheGroup.enforceSizeThreshold;\n\n          // Skip when no chunk selected\n          if (selectedChunks.length === 0) continue;\n          const usedChunks = new Set(selectedChunks);\n\n          // Check if maxRequests condition can be fulfilled\n          if (!enforced && (Number.isFinite(item.cacheGroup.maxInitialRequests) || Number.isFinite(item.cacheGroup.maxAsyncRequests))) {\n            for (const chunk of usedChunks) {\n              // respect max requests\n              const maxRequests = chunk.isOnlyInitial() ? item.cacheGroup.maxInitialRequests : chunk.canBeInitial() ? Math.min(item.cacheGroup.maxInitialRequests, item.cacheGroup.maxAsyncRequests) : item.cacheGroup.maxAsyncRequests;\n              if (isFinite(maxRequests) && getRequests(chunk) >= maxRequests) {\n                usedChunks.delete(chunk);\n              }\n            }\n          }\n          outer: for (const chunk of usedChunks) {\n            for (const module of item.modules) {\n              if (chunk.containsModule(module)) continue outer;\n            }\n            usedChunks.delete(chunk);\n          }\n\n          // Were some (invalid) chunks removed from usedChunks?\n          // => readd all modules to the queue, as things could have been changed\n          if (usedChunks.size < selectedChunks.length) {\n            if (usedChunks.size >= item.cacheGroup.minChunks) {\n              const chunksArr = Array.from(usedChunks);\n              for (const module of item.modules) {\n                addModuleToChunksInfoMap(item.cacheGroup, item.cacheGroupIndex, chunksArr, getKey(usedChunks), module);\n              }\n            }\n            continue;\n          }\n\n          // Create the new chunk if not reusing one\n          if (!isReused) {\n            newChunk = compilation.addChunk(chunkName);\n          }\n          // Walk through all chunks\n          for (const chunk of usedChunks) {\n            // Add graph connections for splitted chunk\n            chunk.split(newChunk);\n          }\n\n          // Add a note to the chunk\n          newChunk.chunkReason = isReused ? \"reused as split chunk\" : \"split chunk\";\n          if (item.cacheGroup.key) {\n            newChunk.chunkReason += ` (cache group: ${item.cacheGroup.key})`;\n          }\n          if (chunkName) {\n            newChunk.chunkReason += ` (name: ${chunkName})`;\n            // If the chosen name is already an entry point we remove the entry point\n            const entrypoint = compilation.entrypoints.get(chunkName);\n            if (entrypoint) {\n              compilation.entrypoints.delete(chunkName);\n              entrypoint.remove();\n              newChunk.entryModule = undefined;\n            }\n          }\n          if (item.cacheGroup.filename) {\n            if (!newChunk.isOnlyInitial()) {\n              throw new Error(\"SplitChunksPlugin: You are trying to set a filename for a chunk which is (also) loaded on demand. \" + \"The runtime can only handle loading of chunks which match the chunkFilename schema. \" + \"Using a custom filename would fail at runtime. \" + `(cache group: ${item.cacheGroup.key})`);\n            }\n            newChunk.filenameTemplate = item.cacheGroup.filename;\n          }\n          if (!isReused) {\n            // Add all modules to the new chunk\n            for (const module of item.modules) {\n              if (typeof module.chunkCondition === \"function\") {\n                if (!module.chunkCondition(newChunk)) continue;\n              }\n              // Add module to new chunk\n              GraphHelpers.connectChunkAndModule(newChunk, module);\n              // Remove module from used chunks\n              for (const chunk of usedChunks) {\n                chunk.removeModule(module);\n                module.rewriteChunkInReasons(chunk, [newChunk]);\n              }\n            }\n          } else {\n            // Remove all modules from used chunks\n            for (const module of item.modules) {\n              for (const chunk of usedChunks) {\n                chunk.removeModule(module);\n                module.rewriteChunkInReasons(chunk, [newChunk]);\n              }\n            }\n          }\n          if (item.cacheGroup.maxSize > 0) {\n            const oldMaxSizeSettings = maxSizeQueueMap.get(newChunk);\n            maxSizeQueueMap.set(newChunk, {\n              minSize: Math.max(oldMaxSizeSettings ? oldMaxSizeSettings.minSize : 0, item.cacheGroup.minSizeForMaxSize),\n              maxSize: Math.min(oldMaxSizeSettings ? oldMaxSizeSettings.maxSize : Infinity, item.cacheGroup.maxSize),\n              automaticNameDelimiter: item.cacheGroup.automaticNameDelimiter,\n              keys: oldMaxSizeSettings ? oldMaxSizeSettings.keys.concat(item.cacheGroup.key) : [item.cacheGroup.key]\n            });\n          }\n\n          // remove all modules from other entries and update size\n          for (const _ref7 of chunksInfoMap) {\n            var _ref8 = _slicedToArray(_ref7, 2);\n            const key = _ref8[0];\n            const info = _ref8[1];\n            if (isOverlap(info.chunks, usedChunks)) {\n              // update modules and total size\n              // may remove it from the map when < minSize\n              const oldSize = info.modules.size;\n              for (const module of item.modules) {\n                info.modules.delete(module);\n              }\n              if (info.modules.size !== oldSize) {\n                if (info.modules.size === 0) {\n                  chunksInfoMap.delete(key);\n                  continue;\n                }\n                info.size = getModulesSize(info.modules);\n                if (info.cacheGroup._validateSize && info.size < info.cacheGroup.minSize) {\n                  chunksInfoMap.delete(key);\n                }\n                if (info.modules.size === 0) {\n                  chunksInfoMap.delete(key);\n                }\n              }\n            }\n          }\n        }\n        const incorrectMinMaxSizeSet = new Set();\n\n        // Make sure that maxSize is fulfilled\n        for (const chunk of compilation.chunks.slice()) {\n          const _ref9 = maxSizeQueueMap.get(chunk) || this.options.fallbackCacheGroup,\n            minSize = _ref9.minSize,\n            maxSize = _ref9.maxSize,\n            automaticNameDelimiter = _ref9.automaticNameDelimiter,\n            keys = _ref9.keys;\n          if (!maxSize) continue;\n          if (minSize > maxSize) {\n            const warningKey = `${keys && keys.join()} ${minSize} ${maxSize}`;\n            if (!incorrectMinMaxSizeSet.has(warningKey)) {\n              incorrectMinMaxSizeSet.add(warningKey);\n              compilation.warnings.push(new MinMaxSizeWarning(keys, minSize, maxSize));\n            }\n          }\n          const results = deterministicGroupingForModules({\n            maxSize: Math.max(minSize, maxSize),\n            minSize,\n            items: chunk.modulesIterable,\n            getKey(module) {\n              const ident = contextify(compilation.options.context, module.identifier());\n              const name = module.nameForCondition ? contextify(compilation.options.context, module.nameForCondition()) : ident.replace(/^.*!|\\?[^?!]*$/g, \"\");\n              const fullKey = name + automaticNameDelimiter + hashFilename(ident);\n              return fullKey.replace(/[\\\\/?]/g, \"_\");\n            },\n            getSize(module) {\n              return module.size();\n            }\n          });\n          results.sort((a, b) => {\n            if (a.key < b.key) return -1;\n            if (a.key > b.key) return 1;\n            return 0;\n          });\n          for (let i = 0; i < results.length; i++) {\n            const group = results[i];\n            const key = this.options.hidePathInfo ? hashFilename(group.key) : group.key;\n            let name = chunk.name ? chunk.name + automaticNameDelimiter + key : null;\n            if (name && name.length > 100) {\n              name = name.slice(0, 100) + automaticNameDelimiter + hashFilename(name);\n            }\n            let newPart;\n            if (i !== results.length - 1) {\n              newPart = compilation.addChunk(name);\n              chunk.split(newPart);\n              newPart.chunkReason = chunk.chunkReason;\n              // Add all modules to the new chunk\n              for (const module of group.items) {\n                if (typeof module.chunkCondition === \"function\") {\n                  if (!module.chunkCondition(newPart)) continue;\n                }\n                // Add module to new chunk\n                GraphHelpers.connectChunkAndModule(newPart, module);\n                // Remove module from used chunks\n                chunk.removeModule(module);\n                module.rewriteChunkInReasons(chunk, [newPart]);\n              }\n            } else {\n              // change the chunk to be a part\n              newPart = chunk;\n              chunk.name = name;\n            }\n          }\n        }\n      });\n    });\n  }\n};","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}